par(mar=c(5,4,2,1))
plot(trees$Height, trees$Volume, pch=19, main="Relationship of tree volume to tree height",
xlab="Height (ft)", ylab="Volume (cubic ft)")
par(mar=c(5,4,2,1))
plot(trees$Height, trees$Volume, pch=19, main="Relationship of tree volume to tree height",
xlab="Height (ft)", ylab="Volume (cubic ft)")
abline(v=80, lty=3)
abline(h=mean(trees$Volume[trees$Height==80]), lty=3)
par(mar=c(5,4,2,1))
plot(trees$Height, trees$Volume, pch=19, main="Relationship of tree volume to tree height",
xlab="Height (ft)", ylab="Volume (cubic ft)")
abline(lm(trees$Volume ~ trees$Height), lty=1, lwd=2)
segments(80, 0, 80, coef(lm(trees$Volume ~ trees$Height)) %*% c(1,80), lty=2)
segments(80, coef(lm(trees$Volume ~ trees$Height)) %*% c(1,80), 0, coef(lm(trees$Volume ~ trees$Height)) %*% c(1,80), lty=2)
text(70, 65, expression(paste("Predicted Volume = ", beta[0], " + ", beta[1], " * Height")))
text(70, 60, paste0("Predicted Volume = ", round(coef(lm(trees$Volume ~ trees$Height))[1], 2), " + ",
round(coef(lm(trees$Volume ~ trees$Height))[2], 2), " * Height"))
text(70, 55, paste0(-87.12 + 1.54*80, " = ", "-87.12 + 1.54 * 80"))
my.lm <- lm(trees$Volume ~ trees$Girth)
par(mar=c(5,4,1,1))
plot(trees$Girth, trees$Volume, ylim=c(-40,80), xlim=c(-5,30),
main = "",
xlab="Diameter of cherry trees", ylab="Volume of cherry trees",
axes=F)
axis(1, at=seq(-5,30,5))
axis(2, at=seq(-40,80,20), las=2)
abline(v=0, lty=2, col="purple")
text(3,60, "y-axis (x = 0)", col="purple")
abline(h=-36.94, lty=3, col="blue")
abline(a = coef(my.lm)[1], b = coef(my.lm)[2])
segments(5, coef(my.lm)[1] + 5*coef(my.lm)[2],
5+3, coef(my.lm)[1] + (5+3)*coef(my.lm)[2],
lwd=3, col="red")
segments(5, coef(my.lm)[1] + 5*coef(my.lm)[2],
5+3, coef(my.lm)[1] + 5*coef(my.lm)[2])
segments(5+3, coef(my.lm)[1] + 5*coef(my.lm)[2],
5+3, coef(my.lm)[1] + (5+3)*coef(my.lm)[2])
text(9,-5, expression(paste(Delta, "y")))
text(6.5,-16, expression(paste(Delta, "x")))
text(9, -30, expression(paste(beta[0], " = -36.94")), col="blue")
text(6, 15, expression(paste(beta[1], " = ", paste(Delta, "y", "/", Delta, "x"), )), col="red")
text(6.1, 3.5, " = 5.07", col="red")
arrows(6,-29, 0,-36.94, angle = 5, col="blue")
points(0,-36.94, col="blue", pch=19)
points(20.6,77, col="brown")
points(20.6,coef(my.lm)[1]+20.6*coef(my.lm)[2], pch=19, col="black")
segments(20.6,77, 20.6,coef(my.lm)[1]+20.6*coef(my.lm)[2], col="green", lwd=3)
text(19.5,72, expression(paste("u"[31])), col="green")
text(23,62, expression(paste("predicted value of ", "y"[31])), cex=0.75, col="black")
text(18,82.5, expression(paste("observed value of ", "y"[31])), cex=0.75, col="brown")
par(mar=c(5,4,2,1))
plot(trees$Height, trees$Volume, pch=19, main="Relationship of tree volume to tree height",
xlab="Height (ft)", ylab="Volume (cubic ft)")
abline(lm(trees$Volume ~ trees$Height), lty=1, lwd=2)
segments(80, 0, 80, coef(lm(trees$Volume ~ trees$Height)) %*% c(1,80), lty=2)
segments(80, coef(lm(trees$Volume ~ trees$Height)) %*% c(1,80), 0, coef(lm(trees$Volume ~ trees$Height)) %*% c(1,80), lty=2)
text(70, 65, expression(paste("Predicted Volume = ", beta[0], " + ", beta[1], " * Height")))
text(70, 60, paste0("Predicted Volume = ", round(coef(lm(trees$Volume ~ trees$Height))[1], 2), " + ",
round(coef(lm(trees$Volume ~ trees$Height))[2], 2), " * Height"))
text(70, 55, paste0(-87.12 + 1.54*80, " = ", "-87.12 + 1.54 * 80"))
my.lm <- lm(trees$Volume ~ trees$Girth)
par(mar=c(5,4,1,1))
plot(trees$Girth, trees$Volume, ylim=c(-40,80), xlim=c(-5,30),
main = "",
xlab="Diameter of cherry trees", ylab="Volume of cherry trees",
axes=F)
axis(1, at=seq(-5,30,5))
axis(2, at=seq(-40,80,20), las=2)
abline(v=0, lty=2, col="purple")
text(3,60, "y-axis (x = 0)", col="purple")
abline(h=-36.94, lty=3, col="blue")
abline(a = coef(my.lm)[1], b = coef(my.lm)[2])
segments(5, coef(my.lm)[1] + 5*coef(my.lm)[2],
5+3, coef(my.lm)[1] + (5+3)*coef(my.lm)[2],
lwd=3, col="red")
segments(5, coef(my.lm)[1] + 5*coef(my.lm)[2],
5+3, coef(my.lm)[1] + 5*coef(my.lm)[2])
segments(5+3, coef(my.lm)[1] + 5*coef(my.lm)[2],
5+3, coef(my.lm)[1] + (5+3)*coef(my.lm)[2])
text(9,-5, expression(paste(Delta, "y")))
text(6.5,-16, expression(paste(Delta, "x")))
text(9, -30, expression(paste(beta[0], " = -36.94")), col="blue")
text(6, 15, expression(paste(beta[1], " = ", paste(Delta, "y", "/", Delta, "x"), )), col="red")
text(6.1, 3.5, " = 5.07", col="red")
arrows(6,-29, 0,-36.94, angle = 5, col="blue")
points(0,-36.94, col="blue", pch=19)
points(20.6,77, col="brown")
points(20.6,coef(my.lm)[1]+20.6*coef(my.lm)[2], pch=19, col="black")
segments(20.6,77, 20.6,coef(my.lm)[1]+20.6*coef(my.lm)[2], col="green", lwd=3)
text(19.5,72, expression(paste("u"[31])), col="green")
text(23,62, expression(paste("predicted value of ", "y"[31])), cex=0.75, col="black")
text(18,82.5, expression(paste("observed value of ", "y"[31])), cex=0.75, col="brown")
library(scatterplot3d)
plot3d <- scatterplot3d(trees$Height, trees$Girth, trees$Volume,
angle=55, scale.y=0.7, pch=16, color ="red", main ="Regression Plane",
xlab = "Height", ylab = "Girth", zlab = "Volume")
my.lm<- lm(trees$Volume ~ trees$Height + trees$Girth)
plot3d$plane3d(my.lm, lty.box = "solid")
detach("package:scatterplot3d", unload=TRUE)
library(patchwork)
library(ggplot2)
source("helper_functions.R")
summary(lm(trees$Volume ~ trees$Girth + trees$Height))
curve(2500*x - 20*x^2, 18, 80)
# estimate b1
b1 <- cov(trees$Volume, trees$Girth) / var(trees$Girth)
# estimate b0
b0 <- mean(trees$Volume) - b1*mean(trees$Girth)
# print estimates
c(b0,b1)
# set seed
set.seed(837)
# true correlation between x1 and x2, sample 1000 from pop
X <- cbind(rep(1,1000),
MASS::mvrnorm(1000,
mu = c(0,0),
Sigma = matrix(c(1,0.5,0.5,1), 2, 2)
)
)
# true betas
B <- c(1, 1, 1)
# true error variance
sigma2 <- 1
# true model generates sampled y from pop
y <- X %*% B + rnorm(1000, 0, sigma2)
library(ggplot2)
source("helper_functions.R")
knitr::opts_chunk$set(echo = TRUE)
M <- matrix(1:16, nrow = 4, ncol = 4, byrow = FALSE)
print(M)
print(M[3, 2])
print(mean(M[4,]))
print(M[M[,4] %in% c(14, 16), ])
knitr::opts_chunk$set(echo = FALSE)
M[3, ] <- M[3, ] * 7
print(M)
# load data, assign to object in enviro "aid"
aid <- read.csv("/Users/kristinamensik/Documents/Duke/2024-2025/Regression/kristinamensik/aid_disasters.csv")
# YOUR CODE HERE
getwd()
# load data, assign to object in enviro "aid"
aid <- read.csv("/Users/kristinamensik/Documents/Duke/2024-2025/Regression/Problem Sets + labs/aid_disasters.csv")
#new variables
aid$gdppc.log <- log(aid$gdppc) # logged GDPPC
aid$oda.log <- log(aid$tot_oda)
getwd()
m1 <- lm(given ~ age + faminc, data = contrib)
contrib <- read.csv("contrib.csv")
getwd()
setwd("~/Documents/Duke/2024-2025/902 Legislators Pre-Ananlysis Plan/Prelim Folder/1-16 Draft")
setwd("~/Documents/Duke/2024-2025/Regression/Problem Sets + labs")
getwd()
contrib <- read.csv("contrib.csv")
View(contrib)
#moving up
simulate_regression <- function(nsims, N) {
# matrix for results
coefs_ests <- matrix(NA, nrow = nsims, ncol = 3)
beta <- c(10, -2, 1)
#for loop
for (i in 1:nsims) {
x_vals <- MASS::mvrnorm(N, c(0,0), matrix(c(1, 0.25, 0.25, 1), 2, 2)) #x1 + x2
X <- cbind(1, x_vals) #w/intercept
# crease error term
u <- rnorm(N, 0, 1)
# Generate y
y <- X %*% beta + u
# ols
B_hat <- solve(t(X) %*% X) %*% t(X) %*% y
#  results
coefs_ests[i, ] <- B_hat
}
return(coefs_ests)
}
coefs_est
simulate_regression <- function(nsims, N) {
# matrix for results
coefs_ests <- matrix(NA, nrow = nsims, ncol = 3)
beta <- c(10, -2, 1)
#for loop
for (i in 1:nsims) {
x_vals <- MASS::mvrnorm(N, c(0,0), matrix(c(1, 0.25, 0.25, 1), 2, 2)) #x1 + x2
X <- cbind(1, x_vals) #w/intercept
# crease error term
u <- rnorm(N, 0, 1)
# Generate y
y <- X %*% beta + u
# ols
B_hat <- solve(t(X) %*% X) %*% t(X) %*% y
#  results
coefs_ests[i, ] <- B_hat
}
return(coefs_ests)
}
simulate_regression <- function(nsims, N, corr, covm = 0.25) {
# results matrix
coefs_ests <- matrix(NA, nrow = nsims, ncol = 3)
beta <- c(10, -2, 1)
#for loop
for (i in 1:nsims) {
x_vals <- MASS::mvrnorm(N, mu = c(0,0), matrix(c(1, covm, covm, 1), nrow = 2)) #x1 + x2
X <- cbind(1, x_vals) #w/intercept
# crease error term
u <- rnorm(N, mean = 0, sd = 1)
# Generate y
y <- X %*% beta + u
# ols
B_hat <- solve(t(X) %*% X) %*% t(X) %*% y
#  results
coefs_ests[i, ] <- as.vector(B_hat)
}
return(coefs_ests)
}
coefs_est
simulate_regression <- function(nsims, N, corr, covm = 0.25) {
# results matrix
coefs_ests <- matrix(NA, nrow = nsims, ncol = 3)
beta <- c(10, -2, 1)
#for loop
for (i in 1:nsims) {
x_vals <- MASS::mvrnorm(N, mu = c(0,0), matrix(c(1, covm, covm, 1), nrow = 2)) #x1 + x2
X <- cbind(1, x_vals) #w/intercept
# crease error term
u <- rnorm(N, mean = 0, sd = 1)
# Generate y
y <- X %*% beta + u
# ols
B_hat <- solve(t(X) %*% X) %*% t(X) %*% y
#  results
coefs_ests[i, ] <- as.vector(B_hat)
}
return(coefs_ests)
}
set.seed(1111)
s_1 <- simulate_regression(nsims = 100, N = 100)
#checking means
summary(s_1)
hundred_means <- colMeans(s_1)
s_1_1k <- simulate_regression(nsims = 1000, N = 100)
thous_means <- colMeans(s_1_1k)
set.seed(2222)
#same model, specify N
N <- 100
beta <- c(10, -2, 1)
x_vals <- MASS::mvrnorm(N, c(0,0), matrix(c(1, 0.25, 0.25, 1), 2, 2))
X <- cbind(1, x_vals) #w/intercept
u <- rnorm(N, 0, 1)
y <- X %*% beta + u
# hand-compute ols
B_hat <- solve(t(X) %*% X) %*% t(X) %*% y
rs <- y - X %*% B_hat
sigsq <- sum(rs^2) / (N - 3)
vcm <- sigsq * solve(t(X) %*% X)
se <- sqrt(diag(vcm))
B_hat
se
#checking means
summary(s_1)
s_1
s_1_1k
#same model, specify N
view(trees)
#same model, specify N
View(trees)
set.seed(2222)
#same model, specify N
N <- 100
beta <- c(10, -2, 1)
x_vals <- MASS::mvrnorm(N, c(0,0), matrix(c(1, 0.25, 0.25, 1), 2, 2))
X <- cbind(1, x_vals) #w/intercept
u <- rnorm(N, 0, 1) #error term
y <- X %*% beta + u
# hand-compute ols
B_hat <- solve(t(X) %*% X) %*% t(X) %*% y
rs <- y - X %*% B_hat #residuals
sigsq <- sum(rs^2) / (N - 3) #residual variance
vcm <- sigsq * solve(t(X) %*% X) #vcov matrix
se <- sqrt(diag(vcm)) #se
B_hat
se
table(yougov$educ)
library(dplyr)
yougov %>%
select(narcurb01, cues) %>%
na.omit() %>%
group_by(cues) %>%
summarise(mean_curb = mean(narcurb01)) %>%
mutate(curb_rel_control = mean_curb - mean_curb[1])
# load data
yougov <- read.csv("/Users/kristinamensik/Documents/Duke/2024-2025/Regression/2016 YouGov_Coded.csv", header=T)
# label conditions
cue_names <- c("No party cues","Unpolarized cues","Polarized cues")
con_dec_names <- c("Disagree w/ decision","Agree w/ decision")
# cue conditions
yougov$cues <- yougov$polcond
yougov$cues <- factor(ifelse(yougov$cues == 0, NA, yougov$cues), levels = 1:3, labels = cue_names)
# decision (like/dislike)
yougov$decision <- factor(ifelse(yougov$ideotreat == 0, NA,
ifelse(yougov$ideotreat == -1 & yougov$ideo %in% c(1,2), 1,
ifelse(yougov$ideotreat == 1 & yougov$ideo %in% c(4,5), 1, 0))),
levels = c(0,1),
labels = con_dec_names)
# make factor variables numeric
yougov$cues <- ifelse(yougov$cues == "No party cues", 0,
ifelse(yougov$cues == "Unpolarized cues", 1, 2))
yougov$decision <- ifelse(yougov$decision == "Disagree w/ decision", 0, 1)
# summarize conditions
table(yougov$cues, yougov$decision, dnn = c("Cues condition", "Decision direction"))
# estimate model
m2a <- lm(narcurb01 ~ as.factor(cues), data = yougov)
summary(m2a)
# estimate model
m2b <- lm(narcurb01 ~ 0 + as.factor(cues),
data = yougov)
summary(m2b)
library(dplyr)
yougov %>%
select(narcurb01, cues) %>%
na.omit() %>%
group_by(cues) %>%
summarise(mean_curb = mean(narcurb01)) %>%
mutate(curb_rel_control = mean_curb - mean_curb[1])
# estimate model
m2c <- lm(narcurb01 ~ decision + as.factor(cues), data = yougov)
summary(m2c)
# estimate model
m2d <- lm(narcurb01 ~ decision*as.factor(cues), data = yougov)
summary(m2d)
# estimate model
m2e <- lm(narcurb01 ~ decision + as.factor(cues) +
age01 + female + black + hisp + educ + incomei01 + know01 + relig01,
data = yougov)
summary(m2e)
table(yougov$educ)
# estimate model
m2f <- lm(narcurb01 ~ decision + as.factor(cues)
+ age01 + female + black + hisp + as.factor(educ)
+ incomei01 + know01 + relig01, data = yougov)
summary(m2f)
anova(m2e, m2f)
anova(m2e, m2f)
m3 <- lm(narcurb01 ~ decision*know01 +
age01 + black + hisp + educ + incomei01 + relig01,
data = yougov)
summary(m3)
library(arm)
# get sims
m3_sims <- sim(m3, n.sims = 10000)
# new data for predictions
nd_3 <- data.frame(decision = c(rep(0,11),rep(1,11)), know01 = seq(0,1,0.1),
age01 = mean(yougov$age01, na.rm = T),
black = 0, hisp = 0, educ = median(yougov$educ, na.rm = T),
incomei01 = mean(yougov$incomei01, na.rm = T),
relig01 = mean(yougov$relig01, na.rm = T))
nd_3 <- cbind(1, nd_3, nd_3$decision*nd_3$know01)
# calculate slopes for each sim
know_sims <- t(sapply(1:10000, function(x) as.matrix(nd_3) %*% m3_sims@coef[x, ]))
par(mar=c(5,4,1,1))
plot(1:11, seq(0,1,0.1), type="n", ylim=c(0,0.6), xlab="Political knowledge", ylab="Expected value of Court curbing",
axes=F)
axis(1, at=1:11, labels=seq(0,1,0.1))
axis(2, at=seq(0,0.6,0.1))
lines(1:11, predict(m3, newdata = nd_3)[1:11], col="blue")
lines(1:11, predict(m3, newdata = nd_3)[12:22], col="red")
lines(1:11, apply(know_sims[,1:11], 2, function(x) quantile(x, 0.025)), lty=3, col="blue")
lines(1:11, apply(know_sims[,1:11], 2, function(x) quantile(x, 0.975)), lty=3, col="blue")
lines(1:11, apply(know_sims[,12:22], 2, function(x) quantile(x, 0.025)), lty=3, col="red")
lines(1:11, apply(know_sims[,12:22], 2, function(x) quantile(x, 0.975)), lty=3, col="red")
legend("topright", c("Disliked decision","Liked decision"), col=c("blue","red"), lty=1, bty="n")
library(marginaleffects)
# effect of decision at values of know01
slopes(m3,
variables = c("decision"),
newdata = datagrid(know01 = seq(0, 1, 0.1)))
# model for all
m4 <- lm(narcurb01 ~ know01 +
age01 + black + hisp + educ + incomei01 + relig01,
data = yougov)
# model for disliked decisions
m4_d <- lm(narcurb01 ~ know01 +
age01 + black + hisp + educ + incomei01 + relig01,
data = subset(yougov, decision == 0))
# model for liked decisions
m4_l <- lm(narcurb01 ~ know01 +
age01 + black + hisp + educ + incomei01 + relig01,
data = subset(yougov, decision == 1))
SSR_R <- sum(m4$residuals^2)
SSR_D <- sum(m4_d$residuals^2)
SSR_L <- sum(m4_l$residuals^2)
Fstat <- ( (SSR_R  - (SSR_D + SSR_L)) / (SSR_D + SSR_L) ) * (nrow(m4$model) - 2*(length(coef(m4)))) / length(coef(m4))
p <- 1 - pf(Fstat, length(coef(m4)), nrow(m4$model) - 2*(length(coef(m4))))
cbind(Fstat, p)
# data
BEPS <- carData::BEPS
BEPS$gender <- ifelse(BEPS$gender == "female", 1, 0)
# estimate model
m1 <- lm(Europe ~ I(age/10) + gender + economic.cond.national + economic.cond.household, data = BEPS)
summary(m1)
library(sandwich)
# adjust SEs
N <- nrow(m1$model)
K <- summary(m1)$df[1]
X <- cbind(rep(1, N), m1$model[,-1])
colnames(X)[1] <- "Intercept"
X <- as.matrix(X)
# default standard error
se_default <- sqrt(diag((sum(m1$residuals^2) / m1$df.residual) * (solve(t(X) %*% X))))
# robust standard error (plug squared residuals in)
se_robust <- sqrt(diag(
solve(t(X) %*% X) %*% t(X) %*%
diag(m1$residuals^2) %*%
X %*% solve(t(X) %*% X)
)
)
# print
round(data.frame(coefficient = coef(m1),
se.default = se_default,
se.robust = se_robust,
se.robust.sandwich = sqrt(diag(sandwich::vcovHC(m1, type = "HC0")))),
5)
# set seed
set.seed(837)
# true correlation between x1 and x2, sample 1000 from pop
X <- cbind(rep(1,1000),
MASS::mvrnorm(1000,
mu = c(0,0),
Sigma = matrix(c(1,0.5,0.5,1), 2, 2)
)
)
# true betas
B <- c(1, 1, 1)
# true error variance
sigma2 <- 1
# true model generates sampled y from pop
y <- X %*% B + rnorm(1000, 0, sigma2)
